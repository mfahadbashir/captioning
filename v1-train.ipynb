{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "from PIL import Image \n",
    "\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform=transforms.Compose([\n",
    "                                transforms.Resize((255,255)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])                               \n",
    "])\n",
    "\n",
    "val_transform=transforms.Compose([\n",
    "                                transforms.Resize((255,255)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])                               \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.45s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 1.3 s, sys: 227 ms, total: 1.53 s\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_list = dset.CocoCaptions(root = '/home/valkyrie/data/2014/train2014/',\n",
    "                        annFile = '/home/valkyrie/data/2014/annotations/captions_train2014.json',\n",
    "                        transform=train_transform)\n",
    "\n",
    "val_list=dset.CocoCaptions(root = '/home/valkyrie/data/2014/val2014/',\n",
    "                        annFile = '/home/valkyrie/data/2014/annotations/captions_val2014.json',\n",
    "                        transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(annon_file,min_threshold,load=True):\n",
    "    if (load):\n",
    "        with open('./word2idx', 'rb') as f:\n",
    "            word2idx=pickle.load(f)\n",
    "        with open('./idx2word', 'rb') as f:\n",
    "            idx2word=pickle.load(f)\n",
    "                \n",
    "    else:\n",
    "        word2idx={}\n",
    "        idx2word={}\n",
    "        idx=0\n",
    "\n",
    "        # Adding Start,End and Unkown Token\n",
    "        word='<start>'\n",
    "        word2idx[word]=idx\n",
    "        idx2word[idx]=word\n",
    "        idx+=1\n",
    "\n",
    "        word='<end>'\n",
    "        word2idx[word]=idx\n",
    "        idx2word[idx]=word\n",
    "        idx+=1\n",
    "\n",
    "        word='<unknown>'\n",
    "        word2idx[word]=idx\n",
    "        idx2word[idx]=word\n",
    "        idx+=1\n",
    "        \n",
    "        word='<pad>'\n",
    "        word2idx[word]=idx\n",
    "        idx2word[idx]=word\n",
    "        idx+=1\n",
    "\n",
    "\n",
    "\n",
    "        coco=COCO(annon_file)\n",
    "        keys=coco.anns.keys()\n",
    "        counter=Counter()\n",
    "        for i,key in enumerate(keys):\n",
    "            caption=coco.anns[key]['caption']\n",
    "            tokens=nltk.tokenize.word_tokenize(caption.lower())\n",
    "            counter.update(tokens)\n",
    "\n",
    "        words = [word for word, count in counter.items() if count >= min_threshold]\n",
    "        for word in words:\n",
    "            if word not in word2idx:\n",
    "                word2idx[word]=idx\n",
    "                idx2word[idx]=word\n",
    "                idx+=1\n",
    "        \n",
    "        with open('./word2idx', 'wb') as f:\n",
    "            pickle.dump(word2idx, f)\n",
    "\n",
    "        with open('./idx2word', 'wb') as f:\n",
    "            pickle.dump(idx2word, f)\n",
    "\n",
    "    return word2idx,idx2word\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coco=COCO(anon_file_train)\n",
    "# ordered = OrderedDict(sorted(coco.anns.items(), key=lambda i: i[1]['image_id']))\n",
    "# keys=list(ordered.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered[keys[2]]['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image,captions=train_list[0]\n",
    "# caps=encode_cap_val(captions,word2idx,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_caption(idx2word,caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cap_val(captions,word2idx,max_len):\n",
    "    \n",
    "    encoded_captions=[]\n",
    "    for cap in captions:\n",
    "        encoded=[]\n",
    "        tokens=nltk.tokenize.word_tokenize(cap.lower())\n",
    "        encoded.append(word2idx['<start>'])\n",
    "        for token in tokens:\n",
    "            if token in word2idx.keys():\n",
    "                encoded.append(word2idx[token])\n",
    "            else:\n",
    "                encoded.append(word2idx['<unknown>'])\n",
    "        encoded.append(word2idx['<end>'])\n",
    "        encoded.extend([word2idx['<pad>']]*(max_len-len(tokens)))\n",
    "        encoded_captions.append(encoded)\n",
    "    return encoded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_captions(annon_file,word2idx,max_len):\n",
    "    coco=COCO(annon_file)\n",
    "    ordered = OrderedDict(sorted(coco.anns.items(), key=lambda i: i[1]['image_id']))\n",
    "    keys=list(ordered.keys())\n",
    "    encoded_captions=[]\n",
    "    lengths=[]\n",
    "    image_ids=[]\n",
    "    \n",
    "    index=0\n",
    "    for key in keys:\n",
    "        encoded=[]\n",
    "        caption=ordered[key]['caption']\n",
    "        image_id=ordered[key]['image_id']\n",
    "        tokens=nltk.tokenize.word_tokenize(caption.lower())\n",
    "        encoded.append(word2idx['<start>'])\n",
    "        for token in tokens:\n",
    "            if token in word2idx.keys():\n",
    "                encoded.append(word2idx[token])\n",
    "            else:\n",
    "                encoded.append(word2idx['<unknown>'])\n",
    "        lengths.append(len(tokens)+2)\n",
    "        encoded.append(word2idx['<end>'])\n",
    "        encoded.extend([word2idx['<pad>']]*(max_len-len(tokens)))\n",
    "        encoded_captions.append(encoded)        \n",
    "        image_ids.append(image_id)\n",
    "\n",
    "    return encoded_captions,lengths,image_ids\n",
    "\n",
    "def decode_caption(idx2word,cap_list):\n",
    "    \n",
    "    decoded_caption=[]\n",
    "    for caption in cap_list:\n",
    "        decoded=[]\n",
    "        for token in caption:\n",
    "            if token==3:\n",
    "                break\n",
    "            decoded.append(idx2word[token])\n",
    "            \n",
    "        decoded_caption.append(decoded)\n",
    "    return decoded_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.27s)\n",
      "creating index...\n",
      "index created!\n",
      "CPU times: user 1min 20s, sys: 675 ms, total: 1min 21s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "anon_file_train='/home/valkyrie/data/2014/annotations/captions_train2014.json'\n",
    "anon_file_val='/home/valkyrie/data/2014/annotations/captions_val2014.json'\n",
    "word2idx,idx2word=build_vocab(anon_file_train,5,load=True)\n",
    "\n",
    "# Train Captions\n",
    "encoded_captions_train,lengths_train,image_ids_train=encode_captions(anon_file_train,word2idx,max_len=100)\n",
    "\n",
    "# Val Captions\n",
    "encoded_captions_val,lengths_val,image_ids_val=encode_captions(anon_file_val,word2idx,max_len=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start>',\n",
       "  'there',\n",
       "  'are',\n",
       "  'containers',\n",
       "  'filled',\n",
       "  'with',\n",
       "  'different',\n",
       "  'kinds',\n",
       "  'of',\n",
       "  'foods',\n",
       "  '<end>']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_caption(idx2word,[encoded_captions_train[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_captions_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414113, 82783, 414113)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_captions_train),len(train_list),len(image_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self,data_list,encoded_captions,lengths,image_ids,val=False):\n",
    "        \n",
    "        self.data=data_list\n",
    "        self.encoded_captions=encoded_captions\n",
    "        self.cap_lens=lengths\n",
    "        self.image_ids=image_ids\n",
    "        self.val=val\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        caption=self.encoded_captions[index]\n",
    "        image_id=self.image_ids[index]\n",
    "        length=self.cap_lens[index]\n",
    "        \n",
    "        idx=self.data.ids.index(image_id)\n",
    "        image,captions=self.data[idx]\n",
    "        \n",
    "        caption=torch.tensor(caption)\n",
    "        length=torch.tensor(length)\n",
    "        \n",
    "        if(self.val):\n",
    "#             print (captions)\n",
    "            captions=captions[:4]\n",
    "            all_caps=encode_cap_val(captions,word2idx,100)\n",
    "            all_caps=torch.LongTensor(all_caps)\n",
    "            return image,caption,length,all_caps \n",
    "        else:\n",
    "            return image,caption,length\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=CaptionDataset(train_list,encoded_captions_train,lengths_train,image_ids_train,val=False)\n",
    "train_loader=torch.utils.data.DataLoader(train_set,batch_size=100)\n",
    "\n",
    "\n",
    "val_set=CaptionDataset(val_list,encoded_captions_val,lengths_val,image_ids_val,val=True)\n",
    "val_loader=torch.utils.data.DataLoader(val_set,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.67 s, sys: 362 ms, total: 6.03 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images,captions,lengths=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 113 ms, total: 11.1 s\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "images,captions,lengths,all_caps=next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 255, 255])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 102])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 102])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_caps[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,base_model,fine_tune=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.base_model = base_model\n",
    "\n",
    "        if (fine_tune==False):\n",
    "            for param in self.base_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((14,14))\n",
    "        \n",
    "    def forward(self,images):\n",
    "        feat = self.base_model.extract_features(images)  \n",
    "        out = self.adaptive_pool(feat)  \n",
    "        out = out.permute(0, 2, 3, 1)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  \n",
    "        self.full_att = nn.Linear(attention_dim, 1)  \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)\n",
    "        alpha = self.softmax(att)  \n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) \n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "\n",
    "\n",
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder which uses Attention Network\n",
    "    \"\"\"\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=1536, dropout=0.5):\n",
    "        \n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # Output Layer\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  \n",
    "        self.init_weights()  \n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)    \n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out) \n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  \n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embeddings\n",
    "        embeddings = self.embedding(encoded_captions) \n",
    "\n",
    "        # Initial Cell and Hidden state\n",
    "        h, c = self.init_hidden_state(encoder_out)  \n",
    "\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        \n",
    "        \n",
    "        # Output Tensors\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "\n",
    "        \"\"\"\n",
    "        Main Decode Step. Output used from the previous timestep and the attention weighted encoding is used.\n",
    "        \"\"\"\n",
    "        for size in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > size for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            # LSTM gate\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  \n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, size, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))\n",
    "            preds = self.fc(self.dropout(h)) \n",
    "            predictions[:batch_size_t, size, :] = preds\n",
    "            alphas[:batch_size_t, size, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meter(object):\n",
    "    '''\n",
    "    Helper Class to store different Metrics\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def accuracy(scores, targets, k):\n",
    "    batch_size = targets.size(0)\n",
    "    _, ind = scores.topk(k, 1, True, True)\n",
    "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
    "    correct_total = correct.view(-1).float().sum()\n",
    "    return correct_total.item() * (100.0 / batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch,experiment):\n",
    "\n",
    "    decoder.train() \n",
    "    encoder.train()\n",
    "\n",
    "    # Network pass time\n",
    "    batch_time = Meter() \n",
    "    # data loading time\n",
    "    data_time = Meter()  \n",
    "    # loss \n",
    "    losses = Meter()  \n",
    "    #Top5\n",
    "    top5accs = Meter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        \n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "        targets = caps_sorted[:, 1:]\n",
    "#         print (targerts.shape)\n",
    "        \n",
    "        # Removing PADS\n",
    "#         print (\"LEN: \",pack_padded_sequence(scores, decode_lengths, batch_first=True).data.shape)\n",
    "            \n",
    "        scores= pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "        targets= pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "\n",
    "        \n",
    "        # Backprop\n",
    "        loss = criterion(scores, targets)\n",
    "        # alpha_c(regularization)\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            \n",
    "        # Updating Metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        \n",
    "        # Logging Metrics on Comet\n",
    "        experiment.log_metric(\"train_epoch\", epoch)\n",
    "        experiment.log_metric(\"train_loss\", losses.avg)\n",
    "        experiment.log_metric(\"train_top5acc\", top5accs.avg)\n",
    "        experiment.log_metric(\"batch_time\", batch_time.avg)\n",
    "        \n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                          loss=losses,\n",
    "                                                                          top5=top5accs))\n",
    "\n",
    "\n",
    "def validate_one(val_loader, encoder, decoder, criterion,experiment):\n",
    "    \n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    decoder.eval()  \n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    batch_time = Meter()\n",
    "    losses = Meter()\n",
    "    top5accs = Meter()\n",
    "    \n",
    "    #True Cap\n",
    "    references = list()  \n",
    "    #Hypoth\n",
    "    hypotheses = list()  \n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
    "\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            caplens = caplens.to(device)\n",
    "\n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            scores_copy = scores.clone()\n",
    "            \n",
    "#             print (\"LEN: \",pack_padded_sequence(scores, decode_lengths, batch_first=True).shape)\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n",
    "            targets= pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n",
    "\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5 = accuracy(scores, targets, 5)\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader),\n",
    "                                                                                loss=losses, top5=top5accs))\n",
    "\n",
    "            allcaps = allcaps[sort_ind]  \n",
    "            for j in range(allcaps.shape[0]):\n",
    "                img_caps = allcaps[j].tolist()\n",
    "                img_captions = list(\n",
    "                    map(lambda c: [w for w in c if w not in {word2idx['<start>'], word2idx['<pad>']}],\n",
    "                        img_caps)) \n",
    "                references.append(img_captions)\n",
    "\n",
    "            # Predictions\n",
    "            _, preds = torch.max(scores_copy, dim=2)\n",
    "            preds = preds.tolist()\n",
    "            temp_preds = list()\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
    "            preds = temp_preds\n",
    "            hypotheses.extend(preds)\n",
    "\n",
    "            # Checking if references are equal to hypotheses\n",
    "            assert len(references) == len(hypotheses)\n",
    "\n",
    "        # BLEU-4 scores\n",
    "        bleu4 = corpus_bleu(references, hypotheses)\n",
    "        \n",
    "        # Bleu1\n",
    "        weights = (1.0/1.0, )\n",
    "        bleu1=corpus_bleu(references, hypotheses, weights)\n",
    "        \n",
    "        \n",
    "        # Logging on Comet\n",
    "        experiment.log_metric(\"val_loss\",losses.avg)\n",
    "        experiment.log_metric(\"val_top5acc\",top5accs.avg)\n",
    "        experiment.log_metric(\"val_blue4\",bleu4)\n",
    "        experiment.log_metric(\"val_blue1\",bleu1)\n",
    "        \n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}, BLEU-1 - {bleu1}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu4,bleu1=bleu1))\n",
    "\n",
    "    return bleu4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all(load=False):\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder,train_decoder\n",
    "    \n",
    "\n",
    "    # CometML \n",
    "    experiment = Experiment(api_key=\"laQIvq6M7DVoWfyJRzWQo3ypT\",\n",
    "                            project_name=\"captioning\", workspace=\"spideysloth\")\n",
    "\n",
    "    tag='Dummy'\n",
    "    experiment.add_tag(tag)\n",
    "    experiment.log_parameter(\"Embedding Dimension\",embeding_dimension)\n",
    "    experiment.log_parameter(\"Attention Dimension\",attention_dimension)\n",
    "    experiment.log_parameter(\"Decoder Dimension\",decoder_dimension)\n",
    "    experiment.log_parameter(\"Dropout\",dropout)\n",
    "    experiment.log_parameter(\"BatchSize\",batch_size)\n",
    "    experiment.log_parameter(\"Encoder Lr\",encoder_lr)\n",
    "    experiment.log_parameter(\"Decoder Lr\",decoder_lr)\n",
    "    experiment.log_parameter(\"Alpha_c\",alpha_c)\n",
    "    experiment.log_parameter(\"EffNet\",\"EffNet-B3\")\n",
    "    experiment.log_parameter(\"FineTune Encoder\",fine_tune_encoder)\n",
    "#     experiment.log_parameter(\"FineTune Encoder\",fine_tune_encoder)\n",
    "    \n",
    "    \n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim=attention_dimension,\n",
    "                                       embed_dim=embeding_dimension,\n",
    "                                       decoder_dim=decoder_dimension,\n",
    "                                       vocab_size=len(word2idx),\n",
    "                                       dropout=dropout)\n",
    "    \n",
    "#     if(train_decoder==False):\n",
    "#         for param in decoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#          decoder_optimizer = None\n",
    "\n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                             lr=decoder_lr)\n",
    "    \n",
    "    model = EfficientNet.from_pretrained('efficientnet-b3')\n",
    "    encoder=Encoder(model,fine_tune=fine_tune_encoder)\n",
    "    \n",
    "    # Load Pre-trained\n",
    "    if(load):\n",
    "        decoder_weights='./checkpoints/decoder_epoch_3_best_b3.pth'\n",
    "        encoder_weights='./checkpoints/encoder_epoch_3_best_b3.pth'\n",
    "\n",
    "\n",
    "        decoder.load_state_dict(torch.load(decoder_weights))\n",
    "        encoder.load_state_dict(torch.load(encoder_weights))\n",
    "\n",
    "        \n",
    "        encoder_optimizer=torch.optim.Adam(list(encoder.parameters()),lr=encoder_lr)\n",
    "    else:\n",
    "        encoder_optimizer =None\n",
    "\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    train_set=CaptionDataset(train_list,encoded_captions_train,lengths_train,image_ids_train,val=False)\n",
    "    train_loader=torch.utils.data.DataLoader(train_set,batch_size=batch_size)\n",
    "\n",
    "\n",
    "    val_set=CaptionDataset(val_list,encoded_captions_val,lengths_val,image_ids_val,val=True)\n",
    "    val_loader=torch.utils.data.DataLoader(val_set,batch_size=batch_size)\n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # One epoch's training\n",
    "        train_one(train_loader=train_loader,\n",
    "              encoder=encoder,\n",
    "              decoder=decoder,\n",
    "              criterion=criterion,\n",
    "              encoder_optimizer=encoder_optimizer,\n",
    "              decoder_optimizer=decoder_optimizer,\n",
    "              epoch=epoch,experiment=experiment)\n",
    "\n",
    "        # One epoch's validation\n",
    "        recent_bleu4 = validate_one(val_loader=val_loader,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder,\n",
    "                                criterion=criterion,experiment=experiment)\n",
    "\n",
    "        # Check if there was an improvement\n",
    "        is_best = recent_bleu4 > best_bleu4\n",
    "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            print (\"***********SAVING****************\")\n",
    "            path='./checkpoints/decoder_epoch_{}_best_b3_fineTune.pth'.format(epoch)\n",
    "            torch.save(decoder.state_dict(), path)\n",
    "            \n",
    "            path='./checkpoints/encoder_epoch_{}_best_b3_fineTune.pth'.format(epoch)\n",
    "            torch.save(encoder.state_dict(), path)\n",
    "            \n",
    "            epochs_since_improvement = 0\n",
    "    return encoder,decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Params\n",
    "embeding_dimension = 512  # Word Embeddings\n",
    "attention_dimension = 1024  # Attention Layers\n",
    "decoder_dimension = 1024  # Decoder\n",
    "dropout = 0.3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "\n",
    "\n",
    "# Train Params\n",
    "start_epoch = 0\n",
    "epochs = 10  \n",
    "epochs_since_improvement = 0 \n",
    "batch_size = 30\n",
    "encoder_lr = 0.00005  \n",
    "decoder_lr = 4e-4  \n",
    "alpha_c = 1.0  \n",
    "best_bleu4 = 0.0\n",
    "print_freq = 50  \n",
    "fine_tune_encoder = False  # fine-tune encoder\n",
    "train_decoder=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/spideysloth/captioning/448636fcbaee4d1490845e2f6677bc2b\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Alpha_c             : 1.0\n",
      "COMET INFO:     Attention Dimension : 1024\n",
      "COMET INFO:     BatchSize           : 30\n",
      "COMET INFO:     Decoder Dimension   : 1024\n",
      "COMET INFO:     Decoder Lr          : 0.0004\n",
      "COMET INFO:     Dropout             : 0.3\n",
      "COMET INFO:     EffNet              : EffNet-B3\n",
      "COMET INFO:     Embedding Dimension : 512\n",
      "COMET INFO:     Encoder Lr          : 5e-05\n",
      "COMET INFO:     FineTune Encoder    : 1\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/spideysloth/captioning/ea4621e3b3dd4d12992c7791044f9983\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b3\n",
      "Epoch: [0][0/13804]\tLoss 9.9843 (9.9843)\tTop-5 Accuracy 0.000 (0.000)\n",
      "Epoch: [0][50/13804]\tLoss 5.7012 (6.8729)\tTop-5 Accuracy 41.287 (33.885)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-36376627f005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-1f14e5b423c7>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(load)\u001b[0m\n\u001b[1;32m     81\u001b[0m               \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m               \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m               epoch=epoch,experiment=experiment)\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# One epoch's validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-8a4235ccaf21>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, experiment)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_optimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m                     )\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder,decoder=train_all(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b1\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b1')\n",
    "encoder=Encoder(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stuff[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/decoder_epoch_10_bleu_0.4.pth'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='./checkpoints/decoder_epoch_{}_bleu_{}.pth'.format(10,0.4)\n",
    "path            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/valkyrie/notebooks/caption\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs=img.unsqueeze(dim=0)\n",
    "output=encoder.forward(stuff[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 14, 14, 1280])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "            # The normalize code -> t.sub_(m).div_(s)\n",
    "        return tensor\n",
    "    \n",
    "# unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "# plt.imshow(unorm(img).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
